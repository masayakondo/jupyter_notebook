{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_reinforcement_learning_chapter3_Q_learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHded1n9ATIPgr6reEpTfJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"k66cS_WVH629"},"source":["# 参考\n","- https://qiita.com/ymd_h/items/c393797deb72e1779269"]},{"cell_type":"code","metadata":{"id":"ZuKKuaFVGEgZ"},"source":["!apt update\n","!apt install xvfb\n","!pip install gym-notebook-wrapper\n","!pip install pyvirtualdisplay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_FPrUDl40do"},"source":["!pip install gym==0.9.7\n","!pip uninstall matplotlib\n","!pip install matplotlib==2.2.5\n","!pip install JSAnimation\n","!pip uninstall pyglet -y\n","!pip install pyglet==1.2.4\n","!conda install -c conda-forge ffmpegy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWOWlZoDEUAo","executionInfo":{"status":"ok","timestamp":1614647272896,"user_tz":-540,"elapsed":49386,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"9cb2e6b8-2581-473c-b699-40b1916702db"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8R3GV7jCp8v","executionInfo":{"status":"ok","timestamp":1614647309487,"user_tz":-540,"elapsed":3947,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"d52bb8df-2be3-405a-b5d5-097477c4b22a"},"source":["# 使用するパッケージの宣言\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import gym\n","from pyvirtualdisplay import Display\n","\n","d = Display()\n","d.start()\n","\n","drive_dir = \"drive/My Drive/Colab Notebooks/rl_data\""],"execution_count":18,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.0)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qmIB05gsHS-M"},"source":["import gnwrapper\n","import gym\n","\n","env = gnwrapper.Monitor(gym.make('CartPole-v1'),directory=\"./\",force=True) # Xvfbが起動される\n","o = env.reset()\n","\n","for _ in range(200):\n","    action = np.random.choice(2)  # 0(カートを左に押す), 1(カートを右に押す)をランダムに返す\n","    o, r, d, i = env.step(action) # 本当はDNNからアクションを入れる\n","    if d:\n","        env.reset()\n","env.display()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI1Z9whUJGYr","executionInfo":{"status":"ok","timestamp":1614647386868,"user_tz":-540,"elapsed":1021,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"9441b307-162f-42ff-89d9-b06bfdffea8b"},"source":["o"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.06797274,  1.13380297, -0.13080762, -1.85412193])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"-kuFfmDxfu8V"},"source":["# 状態を離散化してQ-learningする"]},{"cell_type":"code","metadata":{"id":"izxIZuhUft-w","executionInfo":{"status":"ok","timestamp":1614649816927,"user_tz":-540,"elapsed":502,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# 変数をいろいろ設定\n","ENV = \"CartPole-v0\"\n","NUM_DIZITIZED = 8\n","GAMMA = 0.99\n","ETA = 0.5\n","MAX_STEPS = 200\n","NUM_EPISODES = 1000\n","# env = gnwrapper.Monitor(gym.make(ENV),directory=\"./\",force=True)\n","# observation = env.reset()\n","# print(\"環境初期値\", observation)"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPYKYxVigI38","executionInfo":{"status":"ok","timestamp":1614649819200,"user_tz":-540,"elapsed":518,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["class Agent:\n","    '''CartPoleのエージェントクラスです、棒付き台車そのものになります'''\n","\n","    def __init__(self, num_states, num_actions):\n","        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n","\n","    def update_Q_function(self, observation, action, reward, observation_next):\n","        '''Q関数の更新'''\n","        self.brain.update_Q_table(\n","            observation, action, reward, observation_next)\n","\n","    def get_action(self, observation, step):\n","        '''行動の決定'''\n","        action = self.brain.decide_action(observation, step)\n","        return action\n","\n","class Brain:\n","    '''エージェントが持つ脳となるクラスです、Q学習を実行します'''\n","\n","    def __init__(self, num_states, num_actions):\n","        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n","\n","        # Qテーブルを作成。行数は状態を分割数^（4変数）にデジタル変換した値、列数は行動数を示す\n","        self.q_table = np.random.uniform(low=0, high=1, size=(\n","            NUM_DIZITIZED**num_states, num_actions))\n","\n","\n","    def bins(self, clip_min, clip_max, num):\n","        '''観測した状態（連続値）を離散値にデジタル変換する閾値を求める'''\n","        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","    def digitize_state(self, observation):\n","        '''観測したobservation状態を、離散値に変換する'''\n","        cart_pos, cart_v, pole_angle, pole_v = observation\n","        digitized = [\n","            np.digitize(cart_pos, bins=self.bins(-2.4, 2.4, NUM_DIZITIZED)),\n","            np.digitize(cart_v, bins=self.bins(-3.0, 3.0, NUM_DIZITIZED)),\n","            np.digitize(pole_angle, bins=self.bins(-0.5, 0.5, NUM_DIZITIZED)),\n","            np.digitize(pole_v, bins=self.bins(-2.0, 2.0, NUM_DIZITIZED))\n","        ]\n","        return sum([x * (NUM_DIZITIZED**i) for i, x in enumerate(digitized)])\n","\n","    def update_Q_table(self, observation, action, reward, observation_next):\n","        '''QテーブルをQ学習により更新'''\n","        state = self.digitize_state(observation)  # 状態を離散化\n","        state_next = self.digitize_state(observation_next)  # 次の状態を離散化\n","        Max_Q_next = max(self.q_table[state_next][:])\n","        self.q_table[state, action] = self.q_table[state, action] + \\\n","            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n","\n","    def decide_action(self, observation, episode):\n","        '''ε-greedy法で徐々に最適行動のみを採用する'''\n","        state = self.digitize_state(observation)\n","        epsilon = 0.5 * (1 / (episode + 1))\n","\n","        if epsilon <= np.random.uniform(0, 1):\n","            action = np.argmax(self.q_table[state][:])\n","        else:\n","            action = np.random.choice(self.num_actions)  # 0,1の行動をランダムに返す\n","        return action"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNM062Paicps","executionInfo":{"status":"ok","timestamp":1614649820918,"user_tz":-540,"elapsed":515,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["class Environment:\n","    '''CartPoleを実行する環境のクラスです'''\n","\n","    def __init__(self):\n","        # self.env = gym.make(ENV)  # 実行する課題を設定\n","        self.env = gnwrapper.Monitor(gym.make(ENV),directory=\"./\",force=True) # Xvfbが起動される\n","        num_states = self.env.observation_space.shape[0]  # 課題の状態の数4を取得\n","        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n","        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n","\n","    def run(self):\n","        '''実行'''\n","        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n","        is_episode_final = False  # 最終試行フラグ\n","        frames = []  # 動画用に画像を格納する変数\n","\n","        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n","            observation = self.env.reset()  # 環境の初期化\n","\n","            for step in range(MAX_STEPS):  # 1エピソードのループ\n","\n","                if is_episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n","                    frames.append(self.env.render(mode='rgb_array'))\n","\n","                # 行動を求める\n","                action = self.agent.get_action(observation, episode)\n","\n","                # 行動a_tの実行により、s_{t+1}, r_{t+1}を求める\n","                observation_next, _, done, _ = self.env.step(\n","                    action)  # rewardとinfoは使わないので_にする\n","\n","                # 報酬を与える\n","                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n","                    if step < 195:\n","                        reward = -1  # 途中でこけたら罰則として報酬-1を与える\n","                        complete_episodes = 0  # 195step以上連続で立ち続けた試行数をリセット\n","                    else:\n","                        reward = 1  # 立ったまま終了時は報酬1を与える\n","                        complete_episodes += 1  # 連続記録を更新\n","                else:\n","                    reward = 0  # 途中の報酬は0\n","\n","                # step+1の状態observation_nextを用いて,Q関数を更新する\n","                self.agent.update_Q_function(\n","                    observation, action, reward, observation_next)\n","\n","                # 観測の更新\n","                observation = observation_next\n","\n","                # 終了時の処理\n","                if done:\n","                    print('{0} Episode: Finished after {1} time steps'.format(\n","                        episode, step + 1))\n","                    break\n","\n","            if is_episode_final is True:  # 最終試行では動画を保存と描画\n","                # display_frames_as_gif(frames)\n","                self.env.display()\n","                break\n","\n","            if complete_episodes >= 10:  # 10連続成功なら\n","                print('10回連続成功')\n","                is_episode_final = True  # 次の試行を描画を行う最終試行とする"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhhMa719nlxh"},"source":["# main\n","cartpole_env = Environment()\n","cartpole_env.run()"],"execution_count":null,"outputs":[]}]}