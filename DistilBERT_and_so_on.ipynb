{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistilBERT_and_so_on.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzDt8qvbyj0Glukmg0L/a7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"519cb6c4e98c4af8afcb09c13e834104":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ffd6c9eb4f541bba2a96005f7b8e0ae","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ae1363923d4415db4a6eefc965de642","IPY_MODEL_5edb7ddb87f34cc28d4f27e72f37a7a4"]}},"7ffd6c9eb4f541bba2a96005f7b8e0ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ae1363923d4415db4a6eefc965de642":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cf49f2cc9d444867bba596ff2a5bf1b5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":690,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":690,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_99578177e32d4d428a76d6511ad17d54"}},"5edb7ddb87f34cc28d4f27e72f37a7a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f34638d31fc44566b78a0b9d204b3478","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 690/690 [00:00&lt;00:00, 1.00kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_337fcaf92a3d43ba844ef5068cfcba16"}},"cf49f2cc9d444867bba596ff2a5bf1b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"99578177e32d4d428a76d6511ad17d54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f34638d31fc44566b78a0b9d204b3478":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"337fcaf92a3d43ba844ef5068cfcba16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ff25de89f574b209c681b1c856a9949":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f08ac8026d024db6924ee2b075389a7c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8d0dec2bd7f4ad3ab2dd207abe55ae7","IPY_MODEL_edff8c3205be4b0b8d0a9b4acb4bb20d"]}},"f08ac8026d024db6924ee2b075389a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8d0dec2bd7f4ad3ab2dd207abe55ae7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_21efd5f647654491aa77765a8ac2d3e8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":805658,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":805658,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fb7a73e717aa4a4e9a2ede9f6d0fa595"}},"edff8c3205be4b0b8d0a9b4acb4bb20d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8a5d6b47c5bf4ce7839a2d9ab17c6733","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 806k/806k [03:02&lt;00:00, 4.42kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a4b72e91592469ebe8dd1c9859450fc"}},"21efd5f647654491aa77765a8ac2d3e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fb7a73e717aa4a4e9a2ede9f6d0fa595":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a5d6b47c5bf4ce7839a2d9ab17c6733":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a4b72e91592469ebe8dd1c9859450fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"wDFN1cN7qAFD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1597985640790,"user_tz":-540,"elapsed":34195,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"6413229f-3361-4a95-c21b-d376300662c3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XFtveZd5qTE5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597985721364,"user_tz":-540,"elapsed":78410,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"d78833fd-4ec0-4152-8274-0b7cf90a65da"},"source":["# MeCabとtransformersを用意する\n","!apt install aptitude swig\n","!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n","# 以下で報告があるようにmecab-python3のバージョンを0.996.5にしないとtokezerで落ちる\n","# https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer\n","!pip install mecab-python3==0.996.5\n","!pip install unidic-lite # これないとMeCab実行時にエラーで落ちる\n","!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n","  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n","  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n","  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n","  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n","  swig3.0\n","Suggested packages:\n","  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n","  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n","  libwww-perl xapian-tools swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n","  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n","  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n","  libhttp-message-perl libio-html-perl libio-string-perl\n","  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n","  libsub-name-perl libtimedate-perl liburi-perl libxapian30 swig swig3.0\n","0 upgraded, 23 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 4,978 kB of archives.\n","After this operation, 21.4 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 4,978 kB in 2s (2,651 kB/s)\n","Selecting previously unselected package aptitude-common.\n","(Reading database ... 144487 files and directories currently installed.)\n","Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n","Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n","Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n","Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Selecting previously unselected package libcwidget3v5:amd64.\n","Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n","Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n","Selecting previously unselected package libxapian30:amd64.\n","Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n","Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Selecting previously unselected package aptitude.\n","Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n","Unpacking aptitude (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libhtml-tagset-perl.\n","Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n","Unpacking libhtml-tagset-perl (3.20-3) ...\n","Selecting previously unselected package liburi-perl.\n","Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n","Unpacking liburi-perl (1.73-1) ...\n","Selecting previously unselected package libhtml-parser-perl.\n","Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n","Unpacking libhtml-parser-perl (3.72-3build1) ...\n","Selecting previously unselected package libcgi-pm-perl.\n","Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n","Unpacking libcgi-pm-perl (4.38-1) ...\n","Selecting previously unselected package libfcgi-perl.\n","Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n","Unpacking libfcgi-perl (0.78-2build1) ...\n","Selecting previously unselected package libcgi-fast-perl.\n","Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n","Unpacking libcgi-fast-perl (1:2.13-1) ...\n","Selecting previously unselected package libsub-name-perl.\n","Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n","Unpacking libsub-name-perl (0.21-1build1) ...\n","Selecting previously unselected package libclass-accessor-perl.\n","Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n","Unpacking libclass-accessor-perl (0.51-1) ...\n","Selecting previously unselected package libencode-locale-perl.\n","Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n","Unpacking libencode-locale-perl (1.05-1) ...\n","Selecting previously unselected package libtimedate-perl.\n","Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n","Unpacking libtimedate-perl (2.3000-2) ...\n","Selecting previously unselected package libhttp-date-perl.\n","Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n","Unpacking libhttp-date-perl (6.02-1) ...\n","Selecting previously unselected package libio-html-perl.\n","Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n","Unpacking libio-html-perl (1.001-1) ...\n","Selecting previously unselected package liblwp-mediatypes-perl.\n","Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n","Unpacking liblwp-mediatypes-perl (6.02-1) ...\n","Selecting previously unselected package libhttp-message-perl.\n","Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n","Unpacking libhttp-message-perl (6.14-1) ...\n","Selecting previously unselected package libio-string-perl.\n","Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n","Unpacking libio-string-perl (1.08-3) ...\n","Selecting previously unselected package libparse-debianchangelog-perl.\n","Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n","Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n","Selecting previously unselected package swig3.0.\n","Preparing to unpack .../21-swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../22-swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up libhtml-tagset-perl (3.20-3) ...\n","Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up libencode-locale-perl (1.05-1) ...\n","Setting up libtimedate-perl (2.3000-2) ...\n","Setting up libio-html-perl (1.001-1) ...\n","Setting up aptitude-common (0.8.10-6ubuntu1) ...\n","Setting up liblwp-mediatypes-perl (6.02-1) ...\n","Setting up liburi-perl (1.73-1) ...\n","Setting up libhtml-parser-perl (3.72-3build1) ...\n","Setting up libcgi-pm-perl (4.38-1) ...\n","Setting up libio-string-perl (1.08-3) ...\n","Setting up libsub-name-perl (0.21-1build1) ...\n","Setting up libfcgi-perl (0.78-2build1) ...\n","Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Setting up libclass-accessor-perl (0.51-1) ...\n","Setting up swig (3.0.12-1) ...\n","Setting up libhttp-date-perl (6.02-1) ...\n","Setting up libcgi-fast-perl (1:2.13-1) ...\n","Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n","Setting up libhttp-message-perl (6.14-1) ...\n","Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n","Setting up aptitude (0.8.10-6ubuntu1) ...\n","update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.9)\n","xz-utils is already installed at the requested version (5.2.2-1.3)\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.9)\n","xz-utils is already installed at the requested version (5.2.2-1.3)\n","The following NEW packages will be installed:\n","  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n","The following packages will be REMOVED:\n","  libnvidia-common-440{u} \n","0 packages upgraded, 11 newly installed, 1 to remove and 35 not upgraded.\n","Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n","Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n","Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n","Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n","Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n","Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n","Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n","Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n","Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n","Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n","Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n","Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n","Fetched 29.3 MB in 3s (10.0 MB/s)\n","(Reading database ... 145737 files and directories currently installed.)\n","Removing libnvidia-common-440 (440.100-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package libmagic-mgc.\n","(Reading database ... 145732 files and directories currently installed.)\n","Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmagic1:amd64.\n","Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package file.\n","Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking file (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmecab2:amd64.\n","Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n","Unpacking libmecab2:amd64 (0.996-5) ...\n","Selecting previously unselected package libmecab-dev.\n","Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n","Unpacking libmecab-dev (0.996-5) ...\n","Selecting previously unselected package mecab-utils.\n","Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n","Unpacking mecab-utils (0.996-5) ...\n","Selecting previously unselected package mecab-jumandic-utf8.\n","Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-jumandic.\n","Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-ipadic.\n","Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n","Selecting previously unselected package mecab.\n","Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n","Unpacking mecab (0.996-5) ...\n","Selecting previously unselected package mecab-ipadic-utf8.\n","Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Setting up libmecab2:amd64 (0.996-5) ...\n","Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-utils (0.996-5) ...\n","Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up libmecab-dev (0.996-5) ...\n","Setting up file (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Compiling Juman dictionary for Mecab.\n","reading /usr/share/mecab/dic/juman/unk.def ... 37\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n","reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n","reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n","reading /usr/share/mecab/dic/juman/Special.csv ... 158\n","reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n","reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n","reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n","reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n","reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n","reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n","reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n","reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n","reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n","reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n","reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n","reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up mecab (0.996-5) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-jumandic (7.0-20130310-4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","                            \n","Collecting mecab-python3==0.996.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n","\u001b[K     |████████████████████████████████| 17.1MB 203kB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-0.996.5\n","Collecting unidic-lite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/d2/a4233f65f718f27065a4cf23a2c4f05d8bd4c75821e092060c4efaf28e66/unidic-lite-1.0.7.tar.gz (47.3MB)\n","\u001b[K     |████████████████████████████████| 47.3MB 64kB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-cp36-none-any.whl size=47556595 sha256=903770bb3d5aadb5ae05d8dfaa9a133513c7e982cf156f92bedeefa51b23e525\n","  Stored in directory: /root/.cache/pip/wheels/a8/82/7d/086724645e33a575aafd0b1dae2835c37d2c00c6a0a96ee3a0\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.7\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 28.5MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 50.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=41e1786186347e4be6b59f7a8c0848e5a74340a581c05099222775fcd8a48ef8\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ea3RJ5DvsuKU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1597988478731,"user_tz":-540,"elapsed":639,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"bd84fbaa-7e92-4566-fc02-f3b70498244b"},"source":["import pickle\n","import pandas as pd\n","\n","# データセット格納先\n","drive_dir = \"drive/My Drive/Colab Notebooks/livedoor_data/\"\n","\n","with open(drive_dir + \"livedoor_title_category.pickle\", 'rb') as f:\n","  livedoor_data = pickle.load(f)\n","\n","# livedoor_data.head()\n","\n","# カテゴリーのリストをデータセットから取得\n","categories = list(set(livedoor_data['category']))\n","print(categories)\n","\n","# カテゴリーのID辞書を作成\n","id2cat = dict(zip(list(range(len(categories))), categories))\n","cat2id = dict(zip(categories, list(range(len(categories)))))\n","print(id2cat)\n","print(cat2id)\n","\n","# DataFrameにカテゴリーID列を追加\n","livedoor_data['category_id'] = livedoor_data['category'].map(cat2id)\n","\n","# 念の為シャッフル\n","livedoor_data = livedoor_data.sample(frac=1).reset_index(drop=True)\n","\n","# データセットを本文とカテゴリーID列だけにする\n","livedoor_data = livedoor_data[['title', 'category_id']]\n","# # 本文からURLやタイトルを取り除く\n","# livedoor_data['body'] = livedoor_data['body'].map(lambda x: \"\".join(x.split(\"\\n\")[3:]))\n","livedoor_data.head()"],"execution_count":57,"outputs":[{"output_type":"stream","text":["['kaden-channel', 'dokujo-tsushin', 'peachy', 'movie-enter', 'smax', 'livedoor-homme', 'it-life-hack', 'topic-news', 'sports-watch']\n","{0: 'kaden-channel', 1: 'dokujo-tsushin', 2: 'peachy', 3: 'movie-enter', 4: 'smax', 5: 'livedoor-homme', 6: 'it-life-hack', 7: 'topic-news', 8: 'sports-watch'}\n","{'kaden-channel': 0, 'dokujo-tsushin': 1, 'peachy': 2, 'movie-enter': 3, 'smax': 4, 'livedoor-homme': 5, 'it-life-hack': 6, 'topic-news': 7, 'sports-watch': 8}\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>category_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Twitterでまた事件！　今度はビビる大木が悪口を言うショップ店員に怒り心頭！【話題】</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>テレビ朝日で花見のニュースに津波の映像が流れる放送事故　ネット掲示板は一時騒然</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>【Sports Watch】シャワポワを追いつめた17歳美女とは？</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>千葉県船橋市で毎時5.82マイクロシーベルトを検出</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>人恋しい季節の到来−冬にひとりはさみしいヨネ！</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          title  category_id\n","0  Twitterでまた事件！　今度はビビる大木が悪口を言うショップ店員に怒り心頭！【話題】            0\n","1       テレビ朝日で花見のニュースに津波の映像が流れる放送事故　ネット掲示板は一時騒然            7\n","2             【Sports Watch】シャワポワを追いつめた17歳美女とは？            8\n","3                     千葉県船橋市で毎時5.82マイクロシーベルトを検出            7\n","4                       人恋しい季節の到来−冬にひとりはさみしいヨネ！            1"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"2yhI26mis8gZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597989358956,"user_tz":-540,"elapsed":1161,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"518a12ff-7cf3-4bcd-c828-910181823ae4"},"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(livedoor_data, train_size=0.8)\n","print(\"学習データサイズ\", train_df.shape[0])\n","print(\"テストデータサイズ\", test_df.shape[0])\n","\n","# tsvファイルとして保存する\n","train_df.to_csv(drive_dir + 'train.tsv', sep='\\t', index=False, header=None)\n","test_df.to_csv(drive_dir + 'test.tsv', sep='\\t', index=False, header=None)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["学習データサイズ 5900\n","テストデータサイズ 1476\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XM-Od5Udr0fQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597989750546,"user_tz":-540,"elapsed":966,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# Download from model library from huggingface.co\n","from transformers import AutoModel, AutoTokenizer\n","from transformers.modeling_bert import BertModel\n","from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n","import torchtext\n","\n","tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","\n","\n","# 公式では以下のようにAutoTokenizerでtokenizerを宣言しますが上記のtokenizerと同様なので、上記のtokenizerを使い回す。\n","# distil_tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","\n","def bert_tokenizer(text):\n","  return tokenizer.encode(text, return_tensors='pt')[0]"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"UD-dLd6SsiNV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597989756189,"user_tz":-540,"elapsed":2680,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# torchtextを使って、学習データとテストデータのイテレータを作成\n","\n","TEXT = torchtext.data.Field(sequential=True, tokenize=bert_tokenizer, use_vocab=False, lower=False,\n","                            include_lengths=True, batch_first=True, pad_token=0)\n","LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n","\n","train_data, test_data = torchtext.data.TabularDataset.splits(\n","    path=drive_dir, train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n","\n","BATCH_SIZE = 32\n","train_iter, test_iter = torchtext.data.Iterator.splits((train_data, test_data), batch_sizes=(BATCH_SIZE, BATCH_SIZE), repeat=False, sort=False)"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"naRFVf_Qtnvk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597989848730,"user_tz":-540,"elapsed":3504,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","# DistilBERT\n","# https://github.com/BandaiNamcoResearchInc/DistilBERT-base-jp\n","\n","class DistilBertClassifier(nn.Module):\n","  def __init__(self):\n","    super(DistilBertClassifier, self).__init__()\n","    self.distil_bert = AutoModel.from_pretrained(\"bandainamco-mirai/distilbert-base-japanese\")\n","    # BERTの隠れ層の次元数は768, livedoorニュースのカテゴリ数が9\n","    self.linear = nn.Linear(768, 9)\n","    # 重み初期化処理\n","    nn.init.normal_(self.linear.weight, std=0.02)\n","    nn.init.normal_(self.linear.bias, 0)\n","\n","  def forward(self, input_ids):\n","    # last_hidden_stateとattentionsを受け取る\n","    vec, _ = self.distil_bert(input_ids)\n","    # 先頭トークンclsのベクトルだけ取得\n","    vec = vec[:,0,:]\n","    vec = vec.view(-1, 768)\n","    # 全結合層でクラス分類用に次元を変換\n","    out = self.linear(vec)\n","    return F.log_softmax(out)\n","\n","distil_classifier = DistilBertClassifier()\n","# distil_classifier.distil_bert"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"hv4kW24ltuTv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597990143173,"user_tz":-540,"elapsed":985,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# ファインチューニングの設定\n","# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n","\n","# まずは全部OFF\n","for param in distil_classifier.parameters():\n","    param.requires_grad = False\n","\n","# BERTの最後の層だけ更新ON\n","for param in distil_classifier.distil_bert.transformer.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","# クラス分類のところもON\n","for param in distil_classifier.linear.parameters():\n","    param.requires_grad = True\n","\n","import torch.optim as optim\n","\n","# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする。\n","optimizer = optim.Adam([\n","    {'params': distil_classifier.distil_bert.transformer.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': distil_classifier.linear.parameters(), 'lr': 1e-4}\n","])\n","\n","# 損失関数の設定\n","loss_function = nn.NLLLoss()"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNxJ3QFLvkmm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1597990693450,"user_tz":-540,"elapsed":61120,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"caaebee6-9c2a-404f-a677-806b81f0f8e1"},"source":["import time\n","\n","# GPUの設定\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# ネットワークをGPUへ送る\n","distil_classifier.to(device)\n","losses = []\n","\n","start = time.time()\n","# エポック数は10で\n","for epoch in range(10):\n","  all_loss = 0\n","  for idx, batch in enumerate(train_iter):\n","    batch_loss = 0\n","    distil_classifier.zero_grad()\n","    input_ids = batch.Text[0].to(device)\n","    label_ids = batch.Label.to(device)\n","    out = distil_classifier(input_ids)\n","    batch_loss = loss_function(out, label_ids)\n","    batch_loss.backward()\n","    optimizer.step()\n","    all_loss += batch_loss.item()\n","  print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n","\n","end = time.time()\n","print (\"time : \", end - start)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0 \t loss 450.1027842760086\n","epoch 1 \t loss 317.39041769504547\n","epoch 2 \t loss 211.34138756990433\n","epoch 3 \t loss 144.4813650548458\n","epoch 4 \t loss 106.24609130620956\n","epoch 5 \t loss 83.87273170053959\n","epoch 6 \t loss 68.9661111086607\n","epoch 7 \t loss 59.31868125498295\n","epoch 8 \t loss 49.874382212758064\n","epoch 9 \t loss 41.56027300283313\n","time :  60.22182369232178\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TK9_QCmLv1i1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597990803237,"user_tz":-540,"elapsed":2004,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"4861b83c-1c3c-468f-8aa8-bdbb8b98348c"},"source":["\n","from sklearn.metrics import classification_report\n","\n","answer = []\n","prediction = []\n","with torch.no_grad():\n","    for batch in test_iter:\n","\n","        text_tensor = batch.Text[0].to(device)\n","        label_tensor = batch.Label.to(device)\n","\n","        score = distil_classifier(text_tensor)\n","        _, pred = torch.max(score, 1)\n","\n","        prediction += list(pred.cpu().numpy())\n","        answer += list(label_tensor.cpu().numpy())\n","print(classification_report(prediction, answer, target_names=categories))"],"execution_count":67,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," kaden-channel       0.93      0.96      0.95       163\n","dokujo-tsushin       0.88      0.88      0.88       178\n","        peachy       0.86      0.75      0.80       202\n","   movie-enter       0.86      0.84      0.85       183\n","          smax       0.96      0.95      0.95       165\n","livedoor-homme       0.67      0.71      0.69        96\n","  it-life-hack       0.91      0.91      0.91       178\n","    topic-news       0.80      0.86      0.83       148\n","  sports-watch       0.88      0.91      0.89       163\n","\n","      accuracy                           0.87      1476\n","     macro avg       0.86      0.86      0.86      1476\n","  weighted avg       0.87      0.87      0.87      1476\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d37SYyQYwL7G","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597991209503,"user_tz":-540,"elapsed":3553,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["class BertClassifier(nn.Module):\n","  def __init__(self):\n","    super(BertClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","    # BERTの隠れ層の次元数は768, livedoorニュースのカテゴリ数が9\n","    self.linear = nn.Linear(768, 9)\n","    # 重み初期化処理\n","    nn.init.normal_(self.linear.weight, std=0.02)\n","    nn.init.normal_(self.linear.bias, 0)\n","\n","  def forward(self, input_ids):\n","    # last_hidden_stateとattentionsを受け取る\n","    vec, _ = self.bert(input_ids)\n","    # 先頭トークンclsのベクトルだけ取得\n","    vec = vec[:,0,:]\n","    vec = vec.view(-1, 768)\n","    # 全結合層でクラス分類用に次元を変換\n","    out = self.linear(vec)\n","    return F.log_softmax(out)\n","\n","bert_classifier = BertClassifier()\n","\n","# ファインチューニングの設定\n","# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n","\n","# まずは全部OFF\n","for param in bert_classifier.parameters():\n","    param.requires_grad = False\n","\n","# BERTの最後の層だけ更新ON\n","for param in bert_classifier.bert.encoder.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","# クラス分類のところもON\n","for param in bert_classifier.linear.parameters():\n","    param.requires_grad = True\n","\n","import torch.optim as optim\n","\n","# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする。\n","optimizer = optim.Adam([\n","    {'params': bert_classifier.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': bert_classifier.linear.parameters(), 'lr': 1e-4}\n","])\n","\n","# 損失関数の設定\n","loss_function = nn.NLLLoss()"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdJjRwYIFyvD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1597991313402,"user_tz":-540,"elapsed":102823,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"4615840e-eacb-46af-aaa5-58adb37fcad6"},"source":["import time\n","\n","start = time.time()\n","# GPUの設定\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# ネットワークをGPUへ送る\n","bert_classifier.to(device)\n","losses = []\n","\n","# エポック数は5で\n","for epoch in range(10):\n","  all_loss = 0\n","  for idx, batch in enumerate(train_iter):\n","    batch_loss = 0\n","    bert_classifier.zero_grad()\n","    input_ids = batch.Text[0].to(device)\n","    label_ids = batch.Label.to(device)\n","    out = bert_classifier(input_ids)\n","    batch_loss = loss_function(out, label_ids)\n","    batch_loss.backward()\n","    optimizer.step()\n","    all_loss += batch_loss.item()\n","  print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n","\n","end = time.time()\n","\n","print (\"time : \", end - start)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0 \t loss 251.19750046730042\n","epoch 1 \t loss 110.7038831859827\n","epoch 2 \t loss 82.88570280373096\n","epoch 3 \t loss 67.0771074667573\n","epoch 4 \t loss 56.24497305601835\n","epoch 5 \t loss 42.61423560976982\n","epoch 6 \t loss 35.98485875874758\n","epoch 7 \t loss 25.728398952633142\n","epoch 8 \t loss 20.40780107676983\n","epoch 9 \t loss 16.567239843308926\n","time :  101.97362518310547\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PI3aKdzbRu0l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597991471583,"user_tz":-540,"elapsed":2892,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"5a4ca269-9e6e-49b2-d9d5-a3b5fd4b65df"},"source":["answer = []\n","prediction = []\n","with torch.no_grad():\n","    for batch in test_iter:\n","\n","        text_tensor = batch.Text[0].to(device)\n","        label_tensor = batch.Label.to(device)\n","\n","        score = bert_classifier(text_tensor)\n","        _, pred = torch.max(score, 1)\n","\n","        prediction += list(pred.cpu().numpy())\n","        answer += list(label_tensor.cpu().numpy())\n","print(classification_report(prediction, answer, target_names=categories))"],"execution_count":70,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," kaden-channel       0.94      0.92      0.93       172\n","dokujo-tsushin       0.75      0.86      0.80       156\n","        peachy       0.81      0.68      0.74       211\n","   movie-enter       0.78      0.81      0.80       171\n","          smax       0.98      0.91      0.94       176\n","livedoor-homme       0.68      0.83      0.75        83\n","  it-life-hack       0.79      0.94      0.86       150\n","    topic-news       0.81      0.76      0.78       172\n","  sports-watch       0.89      0.82      0.85       185\n","\n","      accuracy                           0.83      1476\n","     macro avg       0.83      0.84      0.83      1476\n","  weighted avg       0.84      0.83      0.83      1476\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cn_LhDlAVRcz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597992591157,"user_tz":-540,"elapsed":3648,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# https://www.ai-shift.co.jp/techblog/635\n","\n","class BertClassifierRevised(nn.Module):\n","  def __init__(self):\n","    super(BertClassifierRevised, self).__init__()\n","    self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","    # BERTの隠れ層の次元数は768, livedoorニュースのカテゴリ数が9\n","    self.linear = nn.Linear(768*4, 9)\n","    # 重み初期化処理\n","    nn.init.normal_(self.linear.weight, std=0.02)\n","    nn.init.normal_(self.linear.bias, 0)\n","  \n","  def _get_cls_vec(self, vec):\n","    return vec[:,0,:].view(-1, 768)\n","\n","  def forward(self, input_ids):\n","    # last_hidden_stateとattentionsを受け取る\n","    _, _,  hidden_states = self.bert(input_ids, output_hidden_states=True)\n","    # 先頭トークンclsのベクトルだけ取得\n","    vec1 = self._get_cls_vec(hidden_states[-1])\n","    vec2 = self._get_cls_vec(hidden_states[-2])\n","    vec3 = self._get_cls_vec(hidden_states[-3])\n","    vec4 = self._get_cls_vec(hidden_states[-4])\n","    vec = torch.cat([vec1, vec2, vec3, vec4], dim=1)\n","    # 全結合層でクラス分類用に次元を変換\n","    out = self.linear(vec)\n","    return F.log_softmax(out)\n","\n","bert_classifier_revised = BertClassifierRevised()\n","\n","# ファインチューニングの設定\n","# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n","\n","# まずは全部OFF\n","for param in bert_classifier_revised.parameters():\n","    param.requires_grad = False\n","\n","# BERTの最後の層だけ更新ON\n","for param in bert_classifier_revised.bert.encoder.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","for param in bert_classifier_revised.bert.encoder.layer[-2].parameters():\n","    param.requires_grad = True\n","\n","for param in bert_classifier_revised.bert.encoder.layer[-3].parameters():\n","    param.requires_grad = True\n","\n","for param in bert_classifier_revised.bert.encoder.layer[-4].parameters():\n","    param.requires_grad = True\n","\n","# クラス分類のところもON\n","for param in bert_classifier_revised.linear.parameters():\n","    param.requires_grad = True\n","\n","import torch.optim as optim\n","\n","# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする。\n","optimizer = optim.Adam([\n","    {'params': bert_classifier_revised.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': bert_classifier_revised.bert.encoder.layer[-2].parameters(), 'lr': 5e-5},\n","    {'params': bert_classifier_revised.bert.encoder.layer[-3].parameters(), 'lr': 5e-5},\n","    {'params': bert_classifier_revised.bert.encoder.layer[-4].parameters(), 'lr': 5e-5},\n","    {'params': bert_classifier_revised.linear.parameters(), 'lr': 1e-4}\n","])\n","\n","# 損失関数の設定\n","loss_function = nn.NLLLoss()"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgHE_sB4YVHM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1597992744351,"user_tz":-540,"elapsed":149868,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"7fd56268-5655-4e08-834b-e48993d88691"},"source":["import time\n","\n","start = time.time()\n","# GPUの設定\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# ネットワークをGPUへ送る\n","bert_classifier_revised.to(device)\n","losses = []\n","\n","# エポック数は5で\n","for epoch in range(10):\n","  all_loss = 0\n","  for idx, batch in enumerate(train_iter):\n","    batch_loss = 0\n","    bert_classifier_revised.zero_grad()\n","    input_ids = batch.Text[0].to(device)\n","    label_ids = batch.Label.to(device)\n","    out = bert_classifier_revised(input_ids)\n","    batch_loss = loss_function(out, label_ids)\n","    batch_loss.backward()\n","    optimizer.step()\n","    all_loss += batch_loss.item()\n","  print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n","\n","end = time.time()\n","\n","print (\"time : \", end - start)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0 \t loss 196.0047192275524\n","epoch 1 \t loss 75.8067753687501\n","epoch 2 \t loss 42.30751228891313\n","epoch 3 \t loss 16.470114511903375\n","epoch 4 \t loss 7.427484432584606\n","epoch 5 \t loss 2.9392087209271267\n","epoch 6 \t loss 1.5984382012393326\n","epoch 7 \t loss 1.7370687873335555\n","epoch 8 \t loss 0.9278695838729618\n","epoch 9 \t loss 1.499190401067608\n","time :  149.01919651031494\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2DSpAJ0bYkUR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597992798532,"user_tz":-540,"elapsed":2797,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"426656c9-c3e9-40af-c9d5-1c8b836ab710"},"source":["answer = []\n","prediction = []\n","with torch.no_grad():\n","    for batch in test_iter:\n","\n","        text_tensor = batch.Text[0].to(device)\n","        label_tensor = batch.Label.to(device)\n","\n","        score = bert_classifier_revised(text_tensor)\n","        _, pred = torch.max(score, 1)\n","\n","        prediction += list(pred.cpu().numpy())\n","        answer += list(label_tensor.cpu().numpy())\n","print(classification_report(prediction, answer, target_names=categories))"],"execution_count":73,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," kaden-channel       0.80      0.99      0.89       137\n","dokujo-tsushin       0.89      0.86      0.88       183\n","        peachy       0.78      0.82      0.80       168\n","   movie-enter       0.87      0.88      0.87       176\n","          smax       0.95      0.93      0.94       168\n","livedoor-homme       0.72      0.83      0.77        88\n","  it-life-hack       0.95      0.79      0.86       215\n","    topic-news       0.83      0.84      0.83       159\n","  sports-watch       0.92      0.86      0.89       182\n","\n","      accuracy                           0.86      1476\n","     macro avg       0.86      0.87      0.86      1476\n","  weighted avg       0.87      0.86      0.86      1476\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1vppgFZ9b3Cd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597987248330,"user_tz":-540,"elapsed":6267,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["class BertAndDistilBertClassifier(nn.Module):\n","  def __init__(self):\n","    super(BertAndDistilBertClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","    self.distil_bert = AutoModel.from_pretrained(\"bandainamco-mirai/distilbert-base-japanese\")\n","    # BERTの隠れ層の次元数は768, livedoorニュースのカテゴリ数が9\n","    self.linear = nn.Linear(768*2, 9)\n","    # 重み初期化処理\n","    nn.init.normal_(self.linear.weight, std=0.02)\n","    nn.init.normal_(self.linear.bias, 0)\n","  \n","  def _get_cls_vec(self, vec):\n","    return vec[:,0,:].view(-1, 768)\n","\n","  def forward(self, input_ids):\n","    # last_hidden_stateとattentionsを受け取る\n","    bert_vec, _ = self.bert(input_ids)\n","    distil_vec, _ = self.distil_bert(input_ids)\n","    bert_cls_vec = self._get_cls_vec(bert_vec)\n","    distil_cls_vec = self._get_cls_vec(distil_vec)\n","    vec = torch.cat([bert_cls_vec, distil_cls_vec], dim=1)\n","    out = self.linear(vec)\n","    return F.log_softmax(out)\n","\n","\n","bert_and_distil_classifier = BertAndDistilBertClassifier()\n","\n","# ファインチューニングの設定\n","# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n","\n","# まずは全部OFF\n","for param in bert_and_distil_classifier.parameters():\n","    param.requires_grad = False\n","\n","# BERTの最後の層だけ更新ON\n","for param in bert_and_distil_classifier.bert.encoder.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","# DistilBETの最後の層だけ更新ON\n","for param in bert_and_distil_classifier.distil_bert.transformer.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","# クラス分類のところもON\n","for param in bert_and_distil_classifier.linear.parameters():\n","    param.requires_grad = True\n","\n","import torch.optim as optim\n","\n","# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする。\n","optimizer = optim.Adam([\n","    {'params': bert_and_distil_classifier.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': bert_and_distil_classifier.distil_bert.transformer.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': bert_and_distil_classifier.linear.parameters(), 'lr': 1e-4}\n","])\n","\n","# 損失関数の設定\n","loss_function = nn.NLLLoss()"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S0u_sihdbSt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1597987407203,"user_tz":-540,"elapsed":158073,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"9c870d9b-c65d-42c3-f7ef-709042fa0e96"},"source":["import time\n","\n","start = time.time()\n","# GPUの設定\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# ネットワークをGPUへ送る\n","bert_and_distil_classifier.to(device)\n","losses = []\n","\n","# エポック数は5で\n","for epoch in range(10):\n","  all_loss = 0\n","  for idx, batch in enumerate(train_iter):\n","    batch_loss = 0\n","    bert_and_distil_classifier.zero_grad()\n","    input_ids = batch.Text[0].to(device)\n","    label_ids = batch.Label.to(device)\n","    out = bert_and_distil_classifier(input_ids)\n","    batch_loss = loss_function(out, label_ids)\n","    batch_loss.backward()\n","    optimizer.step()\n","    all_loss += batch_loss.item()\n","  print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n","\n","end = time.time()\n","\n","print (\"time : \", end - start)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0 \t loss 253.8527083992958\n","epoch 1 \t loss 108.67510583996773\n","epoch 2 \t loss 80.17743158340454\n","epoch 3 \t loss 61.985263999551535\n","epoch 4 \t loss 49.55005703121424\n","epoch 5 \t loss 37.398751348257065\n","epoch 6 \t loss 29.679549926891923\n","epoch 7 \t loss 20.620460195466876\n","epoch 8 \t loss 14.49152850266546\n","epoch 9 \t loss 11.417884467169642\n","time :  157.13771057128906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RDo42e0Vdn_o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1597987458522,"user_tz":-540,"elapsed":4451,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"7aebb118-2d34-4e92-f2b2-c1c6266ad840"},"source":["answer = []\n","prediction = []\n","with torch.no_grad():\n","    for batch in test_iter:\n","\n","        text_tensor = batch.Text[0].to(device)\n","        label_tensor = batch.Label.to(device)\n","\n","        score = bert_and_distil_classifier(text_tensor)\n","        _, pred = torch.max(score, 1)\n","\n","        prediction += list(pred.cpu().numpy())\n","        answer += list(label_tensor.cpu().numpy())\n","print(classification_report(prediction, answer, target_names=categories))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," kaden-channel       0.95      0.90      0.92       171\n","dokujo-tsushin       0.84      0.80      0.82       191\n","        peachy       0.78      0.69      0.73       178\n","   movie-enter       0.77      0.85      0.81       162\n","          smax       0.95      0.96      0.96       185\n","livedoor-homme       0.80      0.77      0.78       118\n","  it-life-hack       0.79      0.96      0.86       136\n","    topic-news       0.88      0.81      0.84       173\n","  sports-watch       0.87      0.94      0.90       162\n","\n","      accuracy                           0.85      1476\n","     macro avg       0.85      0.85      0.85      1476\n","  weighted avg       0.85      0.85      0.85      1476\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3FV9k3z7fKI2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":132,"referenced_widgets":["519cb6c4e98c4af8afcb09c13e834104","7ffd6c9eb4f541bba2a96005f7b8e0ae","3ae1363923d4415db4a6eefc965de642","5edb7ddb87f34cc28d4f27e72f37a7a4","cf49f2cc9d444867bba596ff2a5bf1b5","99578177e32d4d428a76d6511ad17d54","f34638d31fc44566b78a0b9d204b3478","337fcaf92a3d43ba844ef5068cfcba16","3ff25de89f574b209c681b1c856a9949","f08ac8026d024db6924ee2b075389a7c","e8d0dec2bd7f4ad3ab2dd207abe55ae7","edff8c3205be4b0b8d0a9b4acb4bb20d","21efd5f647654491aa77765a8ac2d3e8","fb7a73e717aa4a4e9a2ede9f6d0fa595","8a5d6b47c5bf4ce7839a2d9ab17c6733","2a4b72e91592469ebe8dd1c9859450fc"]},"executionInfo":{"status":"ok","timestamp":1597987665854,"user_tz":-540,"elapsed":3837,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"9487e3cc-88db-42d9-fcaa-bafc09ffab9d"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"ALINEAR/albert-japanese\")\n"],"execution_count":45,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"519cb6c4e98c4af8afcb09c13e834104","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=690.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ff25de89f574b209c681b1c856a9949","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=805658.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JuFwRpdaapf6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597986213304,"user_tz":-540,"elapsed":25819,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"2f1e76fc-aea6-4cb1-afac-7bae6d4e4270"},"source":["# import lightgbm as lgbm\n","\n","# preds = []\n","# answer = []\n","# for title, cat_id in zip(ensemble_df['title'], ensemble_df['category_id']):\n","#   with torch.no_grad():\n","#     input_ids = tokenizer.encode(title, return_tensors='pt').to(device)\n","#     bert_score = bert_classifier(input_ids)\n","#     _, bert_pred = torch.max(bert_score, 1)\n","#     distil_score = distil_classifier(input_ids)\n","#     _, distil_pred = torch.max(distil_score, 1)\n","#     preds.append([bert_pred.item(), distil_pred.item()])\n","#     answer.append(cat_id)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oyGpU3rhf9dy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1597986217103,"user_tz":-540,"elapsed":1179,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"2e8c1af0-e6d2-4fad-fad3-347dddfdd0d5"},"source":["# import pandas as pd\n","# X = pd.DataFrame(preds, columns=['bert', 'distil'])\n","# lgbm_model = lgbm.LGBMClassifier()\n","# lgbm_model.fit(X, answer)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', learning_rate=0.1, max_depth=-1,\n","               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n","               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n","               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n","               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ZSeyp7ZkgaAv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597986223660,"user_tz":-540,"elapsed":4162,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"fea86ee7-1d16-482d-f2e5-31d6f031a1f5"},"source":["# answer = []\n","# bert_preds = []\n","# distil_preds = []\n","# with torch.no_grad():\n","#     for batch in test_iter:\n","\n","#         text_tensor = batch.Text[0].to(device)\n","#         label_tensor = batch.Label.to(device)\n","\n","#         bert_score = bert_classifier(text_tensor)\n","#         _, bert_pred = torch.max(bert_score, 1)\n","\n","#         distil_score = distil_classifier(text_tensor)\n","#         _, distil_pred = torch.max(distil_score, 1)\n","\n","#         bert_preds += list(bert_pred.cpu().numpy())\n","#         distil_preds += list(distil_pred.cpu().numpy())\n","\n","#         answer += list(label_tensor.cpu().numpy())\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uhGmV7Ughdg6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1597986227261,"user_tz":-540,"elapsed":670,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}},"outputId":"23300c43-9b9b-4124-cd0b-312d31a93350"},"source":["# X = pd.DataFrame()\n","# X['bert'] = bert_preds\n","# X['distil'] = distil_preds\n","# lgbm_pred = lgbm_model.predict(X)\n","# print(classification_report(lgbm_pred, answer, target_names=categories))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," kaden-channel       0.88      0.97      0.92       170\n","dokujo-tsushin       0.91      0.69      0.79       232\n","        peachy       0.60      0.68      0.64       155\n","   movie-enter       0.83      0.86      0.84       155\n","          smax       0.95      0.97      0.96       173\n","livedoor-homme       0.54      0.93      0.68        57\n","  it-life-hack       0.91      0.86      0.88       200\n","    topic-news       0.84      0.79      0.81       156\n","  sports-watch       0.91      0.87      0.89       178\n","\n","      accuracy                           0.84      1476\n","     macro avg       0.82      0.85      0.82      1476\n","  weighted avg       0.85      0.84      0.84      1476\n","\n"],"name":"stdout"}]}]}