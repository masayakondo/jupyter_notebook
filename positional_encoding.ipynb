{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"positional_encoding.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Hp23dZyXDJHJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"41843ca2-8443-42b0-d4ba-58529248f83f","executionInfo":{"status":"ok","timestamp":1573653813502,"user_tz":-540,"elapsed":20286,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4PEvuSxEEOu","colab_type":"code","colab":{}},"source":["# MeCabをcolabで使えるようにする\n","!apt install aptitude\n","!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n","!pip install mecab-python3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlldanyxEHWM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4032bde3-ee6a-4e9e-8b92-786944e01c4c","executionInfo":{"status":"ok","timestamp":1573653887332,"user_tz":-540,"elapsed":887,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# データをDataFrameにまとめる\n","\n","import os\n","from glob import glob\n","import pandas as pd\n","import linecache\n","from tqdm import tqdm_notebook as tqdm\n","\n","drive_dir = \"drive/My Drive/Colab Notebooks/\"\n","categories = [name for name in os.listdir(drive_dir + 'text') if os.path.isdir(drive_dir + \"text/\" +name)]\n","print(categories)\n","\n","\n","import pickle\n","\n","with open(drive_dir + \"livedoor_datasets.pickle\", 'rb') as f:\n","  datasets = pickle.load(f)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['it-life-hack', 'kaden-channel', 'livedoor-homme', 'topic-news', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax', 'movie-enter']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gSgdRw0IE7J7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f4702420-3ea8-4200-d610-39356b5d17e4","executionInfo":{"status":"ok","timestamp":1573653830218,"user_tz":-540,"elapsed":950,"user":{"displayName":"M K","photoUrl":"","userId":"05537230611998731047"}}},"source":["# 形態素解析を定義\n","\n","import MeCab\n","import re\n","\n","tagger = MeCab.Tagger(\"-Owakati\")\n","\n","def make_wakati(sentence):\n","    # MeCabで分かち書き\n","    sentence = tagger.parse(sentence)\n","    # 半角全角英数字除去\n","    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n","    # 記号もろもろ除去\n","    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n","    # スペースで区切って形態素の配列へ\n","    wakati = sentence.split(\" \")\n","    # 空の要素は削除\n","    wakati = list(filter((\"\").__ne__, wakati))\n","    return wakati\n","\n","# 単語数取得\n","word2index = {}\n","# 系列を揃えるためのパディング\n","word2index.update({\"<pad>\":0})\n","\n","for title in datasets[\"title\"]:\n","    wakati = make_wakati(title)\n","    for word in wakati:\n","        if word in word2index: continue\n","        word2index[word] = len(word2index)\n","print(\"vocab size : \", len(word2index))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["vocab size :  13230\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C09IF5ACE_9A","colab_type":"code","colab":{}},"source":["# データのバッチ化\n","\n","from sklearn.model_selection import train_test_split\n","import random\n","from sklearn.utils import shuffle\n","\n","cat2index = {}\n","for cat in categories:\n","    if cat in cat2index: continue\n","    cat2index[cat] = len(cat2index)\n","\n","def sentence2index(sentence):\n","    wakati = make_wakati(sentence)\n","    return [word2index[w] for w in wakati]\n","    \n","def category2index(cat):\n","    return [cat2index[cat]]\n","\n","index_datasets_title_tmp = []\n","index_datasets_category = []\n","max_len = 0\n","for title, category in zip(datasets[\"title\"], datasets[\"category\"]):\n","  index_title = sentence2index(title)\n","  index_category = category2index(category)\n","  index_datasets_title_tmp.append(index_title)\n","  index_datasets_category.append(index_category)\n","  if max_len < len(index_title):\n","    max_len = len(index_title)\n","\n","# 系列の長さを揃えるために短い系列にパディングを追加\n","# seq2seqのときみたいに後ろパディングしたらLSTMの順伝搬の結果が全部同じになってしまったので、前パディングにしたらうまくいった\n","index_datasets_title = []\n","for title in index_datasets_title_tmp:\n","  for i in range(max_len - len(title)):\n","    title.insert(0, 0)\n","#     title.append(0)\n","  index_datasets_title.append(title)\n","\n","  \n","train_x, test_x, train_y, test_y = train_test_split(index_datasets_title, index_datasets_category, train_size=0.7)\n","\n","\n","\n","def train2batch(title, category, batch_size=100):\n","  title_batch = []\n","  category_batch = []\n","  title_shuffle, category_shuffle = shuffle(title, category)\n","  for i in range(0, len(title), batch_size):\n","    title_batch.append(title_shuffle[i:i+batch_size])\n","    category_batch.append(category_shuffle[i:i+batch_size])\n","  return title_batch, category_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAR4QarkFFXV","colab_type":"code","colab":{}},"source":["# モデル定義\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, batch_size=100):\n","        super(LSTMClassifier, self).__init__()\n","        self.batch_size = batch_size\n","        self.hidden_dim = hidden_dim\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n","        self.softmax = nn.LogSoftmax()\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        _, lstm_out = self.lstm(embeds)\n","        bilstm_out = torch.cat([lstm_out[0][0], lstm_out[0][1]], dim=1)\n","        tag_space = self.hidden2tag(bilstm_out)\n","        tag_scores = self.softmax(tag_space.squeeze())\n","        return tag_scores\n","\n","# ハイパーパラメータ、損失関数、最適化など\n","EMBEDDING_DIM = 200\n","HIDDEN_DIM = 300\n","VOCAB_SIZE = len(word2index)\n","TAG_SIZE = len(categories)\n","model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n","loss_function = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]}]}